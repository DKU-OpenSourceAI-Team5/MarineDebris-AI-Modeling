# ğŸŒŠ MarineDebris-AI-Modeling
### í•´ì–‘ ì“°ë ˆê¸° ì´ë¯¸ì§€ ë¶„ë¥˜ AI í•™ìŠµ ëª¨ë¸ êµ¬í˜„
- í•´ì–‘ ì“°ë ˆê¸° ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì¢…ë¥˜ ë³„ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬<br>ì¶”í›„ í•´ì•ˆ/í•´ì–‘ ì“°ë ˆê¸° ìë™ íƒì§€ í”„ë¡œê·¸ë¨, í•´ì–‘ ì“°ë ˆê¸° ë¶„í¬ë„ ìƒì„± í”„ë¡œê·¸ë¨ ë“±ì— í™œìš© ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.<br>
- ver.1(Colab ì‹¤í–‰ ì½”ë“œ)ì™€ ver.2(Pycharm ì‹¤í–‰ ì½”ë“œ) êµ¬í˜„

<br>

## ğŸ¤– Model ì„¤ëª…
### CNN ëª¨ë¸(Convolutional Neural Network, í•©ì„±ê³± ì‹ ê²½ë§)
> ì´ë¯¸ì§€ ì¸ì‹ ë° íŒ¨í„´ ì¸ì‹ê³¼ ê°™ì€ ì‘ì—…ì— íŠ¹í™”ëœ ì‹ ê²½ë§ êµ¬ì¡°<br>ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ í•™ìŠµí•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ ê²€ì¶œ, ì„¸ê·¸ë©˜í…Œì´ì…˜ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì— í™œìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸


- ì£¼ìš” êµ¬ì„± ìš”ì†Œ
  1. í•©ì„±ê³± ì¸µ (Convolutional Layer):<br>
  í•©ì„±ê³± ì—°ì‚°: ì´ë¯¸ì§€ì— ì»¤ë„(í•„í„°)ì„ ì´ë™ì‹œí‚¤ë©° ì§€ì—­ì ì¸ íŠ¹ì§•ì„ ê°ì§€í•œë‹¤. ê° í•©ì„±ê³± ì—°ì‚°ì€ íŠ¹ì •í•œ íŒ¨í„´ì´ë‚˜ ì—£ì§€ì™€ ê°™ì€ ì‹œê°ì  ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.<br>
  ìŠ¤íŠ¸ë¼ì´ë“œ(Striding): ì»¤ë„ì„ ì´ë™ì‹œí‚¤ëŠ” ê°„ê²©ì„ ì˜ë¯¸í•˜ë©°, ìŠ¤íŠ¸ë¼ì´ë“œê°€ í´ìˆ˜ë¡ ì¶œë ¥ í¬ê¸°ëŠ” ì‘ì•„ì§„ë‹¤.
  2. í™œì„±í™” í•¨ìˆ˜ (Activation Function):<br>
  ReLU(Rectified Linear Unit): ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ, ì…ë ¥ì´ ì–‘ìˆ˜ì´ë©´ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ìŒìˆ˜ì¼ ê²½ìš°ì—ëŠ” 0ìœ¼ë¡œ ë§Œë“ ë‹¤. ë¹„ì„ í˜•ì„±ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì´ ë” ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.
  3. í’€ë§ ì¸µ (Pooling Layer):<br>
  í’€ë§ ì—°ì‚°: ê³µê°„ ì°¨ì›ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ë©°, ì£¼ë¡œ ìµœëŒ€ í’€ë§(Max Pooling)ì´ë‚˜ í‰ê·  í’€ë§(Average Pooling)ì´ ì ìš©ëœë‹¤. ì´ë¥¼ í†µí•´ ê³„ì‚° ë¹„ìš©ì„ ê°ì†Œì‹œí‚¤ê³  ì¤‘ìš”í•œ íŠ¹ì§•ì„ ê°•ì¡°í•  ìˆ˜ ìˆë‹¤.
  4. ì™„ì „ ì—°ê²° ì¸µ (Fully Connected Layer):<br>
  Flatten ì¸µ: CNNì˜ ì¶œë ¥ì„ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹œë‹¤.<br>
  ì™„ì „ ì—°ê²° ì¸µ: í¼ì¹œ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢…ì ì¸ ë¶„ë¥˜ë‚˜ íšŒê·€ë¥¼ ìˆ˜í–‰í•œë‹¤.
  5. ì†ì‹¤ í•¨ìˆ˜ (Loss Function) ë° ìµœì í™”:<br>
  ì†ì‹¤ í•¨ìˆ˜: ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ë¡œ, ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì£¼ë¡œ crossentropy ì†ì‹¤ì´ ì‚¬ìš©ëœë‹¤. ë”°ë¼ì„œ í•´ë‹¹ ëª¨ë¸ì—ì„œë„ crossentropyê°€ ì‚¬ìš©ë˜ì—ˆë‹¤.<br>
  ìµœì í™” ì•Œê³ ë¦¬ì¦˜: ê²½ì‚¬ í•˜ê°•ë²•(GD)ì˜ ë³€í˜•ìœ¼ë¡œ, ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì†ì‹¤ì„ ìµœì†Œí™”í•œë‹¤. í•´ë‹¹ ëª¨ë¸ì—ì„œëŠ” adamì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 


<br>

## ğŸ“™ ver.1: Colab ì‹¤í–‰ ì½”ë“œ
### ê¸°ë³¸ ì •ë³´
- ì‹¤í–‰ OS: Window 11
- python version: 3.10.12
- required libraries: json, pathlib, numpy, tensorflow, keras, sklearn, PIL


### ì‹¤í–‰ ë°©ë²•
1. Import Libraries
```python
# ì½”ë© ë“œë¼ì´ë¸Œ ì—°ê²°í•˜ê¸°
from google.colab import drive
drive.mount('/content/drive')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import json
from pathlib import Path
import numpy as np
import tensorflow as tf
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import LabelBinarizer
from PIL import Image
from keras import models, layers
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
```
2. Data Load
```python
# JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
json_directory_path = '/content/drive/MyDrive/ColabNotebooks/2023/opensource/trash_label/'

# ì´ë¯¸ì§€ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
image_directory_path = '/content/drive/MyDrive/ColabNotebooks/2023/opensource/trash_image/'

# ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
images = []
labels = []

# ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ì„ ì½ì–´ì˜¤ê¸° ìœ„í•œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
data_size = 1000
json_files = list(Path(json_directory_path).rglob('*.json'))[:data_size]

for json_file in json_files:
    with open(json_file, 'r') as f:
        data = json.load(f)

        # ì´ë¯¸ì§€ ê²½ë¡œ
        image_path = Path(image_directory_path) / data['imagePath']

        # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
        image = np.array(Image.open(image_path))

        # ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
        shapes = data.get('shapes', [])
        for shape in shapes:
            label = shape['label']
            points = shape['points']

            images.append(image)
            labels.append(label)
```
3. Pre Processing
```python
# ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜
images = np.array(images)
labels = np.array(labels)

# ë¼ë²¨ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ë¼ë²¨ ì¸ì½”ë”©)
label_binarizer = LabelBinarizer()
labels_encoded = label_binarizer.fit_transform(labels)
```
4. Data Split
```python
# ë°ì´í„° ë¶„í• 
train_images, test_images, train_labels, test_labels = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)

# ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
target_image_size = (224, 224)
train_images_resized = [tf.image.resize(image, target_image_size) for image in train_images]
test_images_resized = [tf.image.resize(image, target_image_size) for image in test_images]

# ì´ë¯¸ì§€ë¥¼ TensorFlow í…ì„œë¡œ ë³€í™˜
train_images_tensor = tf.convert_to_tensor(train_images_resized, dtype=tf.float32)
test_images_tensor = tf.convert_to_tensor(test_images_resized, dtype=tf.float32)

# ë¼ë²¨ì„ TensorFlow í…ì„œë¡œ ë³€í™˜
train_labels_tensor = tf.convert_to_tensor(train_labels, dtype=tf.float32)
test_labels_tensor = tf.convert_to_tensor(test_labels, dtype=tf.float32)
```
5. Define Model - CNN
```python
# CNN ëª¨ë¸ ìƒì„±
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(target_image_size[0], target_image_size[1], 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(len(label_binarizer.classes_), activation='softmax'))

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```
6. Model Training
```python
# ëª¨ë¸ í›ˆë ¨
epochs = 5
batch_size = 16

history = model.fit(
    train_images_np, train_labels,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(test_images_np, test_labels)
)
```
7. Model Test
```python
# ì •í™•ë„ í™•ì¸
test_loss, test_accuracy = model.evaluate(test_images_np, test_labels)
print('\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')
print(f'Dataset Size: {data_size}')
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')
print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')
```

<br>

## ğŸ“™ ver.2: Pycharm(conda) ì‹¤í–‰ ì½”ë“œ
### ê¸°ë³¸ ì •ë³´
- ì‹¤í–‰ OS: macOS
- python version: 3.11
- required libraries: numpy, tensorflow, sklearn(scikit-learn), PIL(Pillow)


### ì‹¤í–‰ ë°©ë²•
1. Import Libraries
```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import json  # JSON íŒŒì¼ ì²˜ë¦¬
from pathlib import Path  # íŒŒì¼ ê²½ë¡œ ê´€ë¦¬
import numpy as np  # ë°°ì—´ ì²˜ë¦¬
import tensorflow as tf  # ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split  # ë°ì´í„° ë¶„í• 
from sklearn.preprocessing import LabelBinarizer  # ë¼ë²¨ ì¸ì½”ë”©
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬
from keras.models import Sequential  # Sequential ëª¨ë¸
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense  # CNN ë ˆì´ì–´ë“¤
```
2. Data Load
```python
# JSON íŒŒì¼ê³¼ ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
json_directory_path = './data/label/'
image_directory_path = './data/image/'

# ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ì„ ì½ì–´ì˜¤ê¸° ìœ„í•œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
data_size = 1500
json_files = list(Path(json_directory_path).rglob('*.json'))[:data_size]

# ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
data_pairs = []

# JSON íŒŒì¼ë“¤ì„ ìˆœíšŒí•˜ë©° ì´ë¯¸ì§€ì™€ ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
for json_file in json_files:
    with open(json_file, 'r') as f:
        data = json.load(f)

        # ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
        image_path = Path(image_directory_path) / data['imagePath']

        # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
        image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
        image_array = tf.keras.preprocessing.image.img_to_array(image)

        # ë¼ë²¨ ì •ë³´ ì¶”ì¶œí•˜ì—¬ ë°ì´í„° í˜ì–´ì— ì¶”ê°€
        shapes = data.get('shapes', [])
        for shape in shapes:
            label = shape['label']
            data_pairs.append((image_array, label))
```
3. Pre Processing
```python
# ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ê°ê° NumPy ë°°ì—´ë¡œ ë³€í™˜
images = np.array([pair[0] for pair in data_pairs])
labels = np.array([pair[1] for pair in data_pairs])

# ë¼ë²¨ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (ë¼ë²¨ ì¸ì½”ë”©)
label_binarizer = LabelBinarizer()
labels_encoded = label_binarizer.fit_transform(labels)

# ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
target_image_size = (224, 224)
images_resized = [tf.image.resize(image, target_image_size) for image in images]

# ì´ë¯¸ì§€ë¥¼ TensorFlow í…ì„œë¡œ ë³€í™˜
images_tensor = tf.convert_to_tensor(images_resized, dtype=tf.float32)

# TensorFlow í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
images_np = images_tensor.numpy()
```
4. Data Split
```python
# ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
train_images_np, test_images_np, train_labels, test_labels = train_test_split(images_np, labels_encoded, test_size=0.2, random_state=42)
```
5. Define Model - CNN
```python
# CNN ëª¨ë¸ ìƒì„±
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(target_image_size[0], target_image_size[1], 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(len(label_binarizer.classes_), activation='softmax'))

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```
6. Model Training
```python
# ëª¨ë¸ í›ˆë ¨
epochs = 5
batch_size = 16

history = model.fit(
    train_images_np, train_labels,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(test_images_np, test_labels)
)
```
7. Model Test
```python
# ì •í™•ë„ í™•ì¸
test_loss, test_accuracy = model.evaluate(test_images_np, test_labels)
print('\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')
print(f'Dataset Size: {data_size}')
print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_accuracy:.4f}')
print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')
```

<br>

## ğŸ“Š ì‹¤í–‰ ê²°ê³¼
### ver.1 ê²°ê³¼
&nbsp;&nbsp;**1. Dataset í¬ê¸° = 500**<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Loss: 1.6855<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Accuracy: 0.4420

<img width="658" alt="KakaoTalk_Photo_2023-12-22-20-17-06 001png" src="https://github.com/DKU-OpenSourceAI-Team5/MarineDebris-AI-Modeling/assets/86196342/6f8828e5-6aea-4462-bf38-66064ba673fc"><br>



&nbsp;&nbsp;**2. Dataset í¬ê¸° = 1000**<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Loss: 1.1954<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Accuracy: 0.5631

<img width="680" alt="KakaoTalk_Photo_2023-12-22-20-17-06 002png" src="https://github.com/DKU-OpenSourceAI-Team5/MarineDebris-AI-Modeling/assets/86196342/5eff2e36-fb21-4350-b1ef-32cb6c180cea"><br>



&nbsp;&nbsp;**3. Dataset í¬ê¸° = 1500**<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Loss: 1.1089<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Accuracy: 0.5704

<img width="660" alt="KakaoTalk_Photo_2023-12-22-20-17-06 003png" src="https://github.com/DKU-OpenSourceAI-Team5/MarineDebris-AI-Modeling/assets/86196342/4e20f0a6-9721-42ff-a223-cdee266b1b6e"><br>



<br>

### ver.2 ê²°ê³¼
&nbsp;&nbsp;**1. Dataset í¬ê¸° = 500**<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Loss: 1.8764<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Accuracy: 0.4229

<img width="1202" alt="pycharm_500" src="https://github.com/DKU-OpenSourceAI-Team5/MarineDebris-AI-Modeling/assets/86196342/c887ad8c-f56a-48ba-951d-b4aee2fb5b1b"><br>


&nbsp;&nbsp;**2. Dataset í¬ê¸° = 1000**<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Loss: 1.7778<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Accuracy: 0.4191

<img width="1145" alt="pycharm_1000" src="https://github.com/DKU-OpenSourceAI-Team5/MarineDebris-AI-Modeling/assets/86196342/710a9241-727c-44b0-9d2f-210600698420"><br>


&nbsp;&nbsp;**3. Dataset í¬ê¸° = 1500**<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Loss: 1.8276<br>
&nbsp;&nbsp;&nbsp;&nbsp;â–¶ï¸ Total Accuracy: 0.4520

<img width="1146" alt="pycharm_1500" src="https://github.com/DKU-OpenSourceAI-Team5/MarineDebris-AI-Modeling/assets/86196342/3774c58c-b076-45f8-8d50-8df2c1c7a945">

